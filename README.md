# 자소서 종합 플랫폼 서비스 'Zazinmori'
## 부제 : 자소서를 진짜 모르는 이들을 위하여..

## 1. 프로젝트 배경 및 목표
- 프로젝트 주제를 정할 때, 공통된 경험을 바탕으로 아이디어를 도출하려고 노력
- 팀원 모두 취준생이라는 점과 자소서라는 공통된 경험을 바탕으로 자소서에 대한 서비스로 의견 종합
- 핵심 서비스는 우리들이 자소서를 쓰면서 가졌던 pain point들을 종합
  - 가령 기업을 둘러 싼 최신 토픽들이 무엇인지 매번 찾아보는게 힘들고 
  - 일일이 합격자소서들을 찾아보는 것도 힘들고 
  - 기업의 기본 정보들을 모든 기업별로 찾아보는 것 또한 힘듦
- 이러한 정보들을 제공해주는 서비스들이 있긴하지만, 하나에 특화되어있거나 부분적으로 제공하고있는 상황
- 이를 종합하여 하나의 서비스로 만들어 낸다면, 자소서를 작성하는 시간이 크게 절약될 뿐만아니라 자소서의 질도 향상될수 있다고 판단
- 이를 근거로 하나의 웹서비스로 기업의 모든 정보를 찾아볼 수 있고 구직자들은 하나의 웹페이지에서 자소서를 작성하기만 되는 자소서 종합 플랫폼을 기획

## 2. 서비스 구상
- 기본 기능
  - 회원가입
  - 로그인/로그아웃

- 자소서 관련 기능
  - 뉴스 토픽 분석
  - 합격 자소서 키워드 분석
  - 기업 세부 정보
    - 기업 기본 정보
    - 기업 재무 정보
    - 기업 채용 공고
  - 자소서 항목별 추천 키워드 분석
  - 유저 맞춤형 기업 추천

- 유저 편의 기능
  - 기업 검색 기능
  - 커뮤니티 기능
  - 이용자 맞춤 서비스
  - 채용 공고 스크랩
  - 자소서 작성 및 저장
  - 유저 맞춤 인기 검색 순위

## 3. 아키텍쳐
- 파이프라인
  - ETL 파이프라인
  - 유저 로그 파이프라인
- WAS
  - Django
  - FastAPI
- Load balancing
  - Nginx
- Clustering
  - Hadoop
  - Spark
  - Kafka 
- ELK Stack
  - Logstash
  - Elasticsearch
  - Kibana
- CI/CD
  - Jenkins
  - Gitlab
- 배포
  - AWS
  - Gunicorn

## 4. 나의 프로젝트 수행 내용
1. 데이터 수집
   - API
     - 기업 관련 데이터
   - Crawling : Multi-processing 사용
     - 잡코리아 합격자소서
     - 링커리어 합격자소서
     - 빅카인즈 기업 관련 뉴스

2. 데이터 가공 및 적재
   - 모든 처리는 pyspark 파일로 작성하여 spakr-submit으로 실행
   - 기업 고유번호를 기준으로 모든 기업 데이터 매핑
     - 기업 명으로 검색시, 그 기업에 대한 모든 데이터를 띄워주기 위함
     - 기업 고유번호가 없는 데이터의 경우, 기업 명을 기준으로 매핑하여 기업 고유번호 컬럼 생성
   - 모든 합격자소서 데이터의 컬럼을 동일하게 처리
     - 값이 없는 경우 빈 문자열로 처리
   - MySQL에 데이터 적재

3. ETL 파이프라인 운영
   - Airflow로 앞선 ETL 프로세스 자동화
     - DAG1. 
     - DAG2.
     - DAG3.
     - DELETE

4. 검색엔진 구현
  - MySQL에서 종합 기업 정보 데이터를 로그스태시로 가져와서 엘라스틱서치에 적재
    - 기업데이터들이 Airflow 상에서 Monthly로 업데이트 되도록 스케쥴링해놨기 때문에, 로그스태시로 기업 종합 정보 데이터도 로그스태시로 Monthly마다 엘라스틱 서치에 업데이트 하도록 스케쥴링
    - 엘라스틱 서치 색인
      - Nori 토크나이저 사용
        - 한글 형태소 분석을 위함
      - 동의어 필터 적용

5. 유저 로그 파이프라인 운영
  - 두 개의 로그스태시 사용
    - 로그 수집기
      - Django에서 발생하는 로그 수집하여 kafka에 전달
    - 메세징 처리기
      - kafka의 topic을 가져와서 메세징 처리후 엘라스틱 서치에 적재
  - 로그 데이터 중간 저장소로 kafka 사용
    - 배포서버, 로그스태시 서버, 카프카 서버, 엘라스틱서치 서버가 다 달랐기 때문에 여러 장애로 인한 데이터 유실문제 고려
    - kafka를 중간저장소로 사용하여, 유저 로그 파이프라인의 안정성 높힘
  - 유저 로그 모니터링
    - 수집된 로그 데이터는 엘라스틱 서치에 적재
    - 이를 키바나로 시각화하여 로그 데이터를 모니터링
      - 검색 키워드를 통해 검색 엔진 성능 향상 목표
      - 유저 사용 패턴을 통한 유저친화적 웹 서비스 개선 목표
      - 유저 맞춤형 추천 서비스 업그레이드 목표

6. 검색 순위 구현
   - 회원 가입 시, 희망 직군과 희망 근무지역을 받음
     - 이를 통해 같은 직군이나 지역을 희망하는 사용자들의 일주일 간의 검색 순위를 표시
     - 로그 파이프 라인으로 수집한 이용자의 검색 키워드 데이터를 토대로 구현

## 5. 사용 데이터 및 ERD
- 데이터 출처
  - API
    - 공공데이터포털
      - 금융위원회 기업기본정보
      - 금융위원회 기업재무정보
      - 금융위원회 기업지배구조정보
    - OPEN DART
      - 금융감독원 단일회사 주요계정
      - 금융감독원 기업개황
      - 금융감독원 기업고유번호
  - Crawling
    - 잡코리아
      - 합격자소서
    - 링커리어
      - 합격자소서
    - 독취사
      - 합격자소서
    - 자소설닷컴
      - 합격자소서
    - 빅카인즈
      - 기업별 주요 뉴스
- ERD
  - 